\item \defpoints{20} [Math review(Information Theory)] Example of joint entropy. Let $p(x, y)$ be given by

\begin{table*}[!htbp]
    \centering
    \begin{tabular}{c|cc}
        \diagbox{$X$}{$Y$} & $0$ & $1$ \\
        \hline $0$ & $\frac{1}{3}$ & $\frac{1}{3}$  \\
        $1$ & 0 & $\frac{1}{3}$  \\
        \hline
    \end{tabular}
\end{table*}

Find:
\begin{itemize}
    \item[(a)] $H(X), H(Y)$. ~\defpoints{4}
    \item[(b)] $H(X \mid Y), H(Y \mid X)$. ~\defpoints{4}
    \item[(c)] $H(X, Y)$. ~\defpoints{3}
    \item[(d)] $H(Y)-H(Y \mid X)$. ~\defpoints{1}
    \item[(e)] $I(X ; Y)$. ~\defpoints{1}
    \item[(f)] Draw a Venn diagram for the quantities in parts (a) through (e). ~\defpoints{3}
    \item[(g)] When is the mutual information $I(X;Y)=0$?  ~\defpoints{4}
\end{itemize}

\textcolor{blue}{Solution} \\













\newpage