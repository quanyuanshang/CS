{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6280b8ea",
      "metadata": {
        "id": "6280b8ea"
      },
      "source": [
        "# HW6 Coding Part"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1a3322",
      "metadata": {
        "id": "ba1a3322"
      },
      "source": [
        "## Problem 0: Set up (0 points)\n",
        "\n",
        "The following code generates a set of sample points from a two-dimensional Gaussian distribution and displays them as a scatter plot. The distribution is centered at (1.0, 3.0), with its first 2 principal direction are shown in the figure. The standard deviation along this direction is 3, and 1 in the orthogonal direction, resulting in an elliptical spread of the samples that exhibits clear directional structure and scale difference. Here we use PC as the abbreviation for principal component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3317fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import generate_data\n",
        "samples = generate_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0c73ca",
      "metadata": {},
      "source": [
        "## Problem 1: PCA (5 points)\n",
        "\n",
        "Now we aim to manually implement PCA using the `samples` data generated in `Problem 0`, and demonstrate the so-called \"dimensionality reduction\" process.\n",
        "\n",
        "Although in this case the data remains two-dimensional (i.e., 2D to 2D) and no actual reduction in dimensionality occurs, we still refer to it as \"dimensionality reduction\" to stay consistent with the terminology used in later sections where true reduction will be performed.\n",
        "\n",
        "In this problem, your are required to implement PCA by yourself, you can use the `numpy` library to help you with the computation, but you are not allowed to use any built-in functions or libraries that directly perform PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3531ac70",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def PCA(data: np.ndarray, n_components: int = 2):\n",
        "    # 1. Center the data\n",
        "    # TODO: Your code here\n",
        "\n",
        "\n",
        "    # 2. Use SVD to compute the eigenvalues and eigenvectors\n",
        "    # TODO: Your code here\n",
        "\n",
        "\n",
        "    # 3. Collect the top 'n_components' eigenvectors, and then project the data onto them, and get the projected_data\n",
        "    # TODO: Your code here\n",
        "\n",
        "\n",
        "    return projected_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ed9960",
      "metadata": {},
      "source": [
        "After implement the PCA algorithm, we can visualize whether the data is successfully reduced to 2D and then project the data onto the principal axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87632908",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "projected_data = PCA(samples, n_components=2)\n",
        "\n",
        "plt.scatter(projected_data[:, 0], projected_data[:, 1], s=8, color='0.65')\n",
        "plt.title(\"After PCA (still 2-D)\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\", rotation=0, labelpad=15)\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0071f75",
      "metadata": {},
      "source": [
        "## Problem 2: Visualize Latent Space (15 points)\n",
        "\n",
        "PCA is a commonly used technique for reducing high-dimensional data into two or three dimensions for visualization. In the context of neural networks, it can help us understand what high-level features (feature maps) have been learned by projecting them onto a space we can see and interpret.\n",
        "\n",
        "In this example, we use the LeNet architecture mentioned in Recitation 8 (you can check the network details in utils, or refer to the files on [piazza resources](https://piazza.com/shanghaitech.edu.cn/spring2025/cs182/resources) about recitation 8). After passing through several convolution and pooling layers, each input image is transformed by the fully connected layer fc1 into a high-dimensional representation known as the latent space. This is followed by another fully connected layer fc2 that maps the latent space features to a 10-dimensional output for classification.\n",
        "\n",
        "A well-trained classifier typically extracts highly informative features in the latent space — that is, the latent features should reflect the class of the input image well. However, since the latent space is often high-dimensional (e.g., 64 dim in ours example), we can’t visualize it directly. To address this, we use PCA to reduce the features to 2D.\n",
        "\n",
        "If your PCA implementation is correct and the classifier is effective, you should observe that the projected features form clear clusters in 2D space, where samples with the same label are close together and samples with different labels are well separated. We have already trained a well-behavior classifier for you, you can just load the model 'mnist_cnn.pth' to get the models parameters.\n",
        "\n",
        "Now, please follow the instructions and complete the code to implement this visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73596b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import CNN\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- load checkpoint ----------\n",
        "# TODO: Load the checkpoint of the CNN model, please make sure you are using a suitable device and model mode\n",
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- prepare dataset ----------\n",
        "dataloader = torch.utils.data.DataLoader(datasets.MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor()), batch_size=256, shuffle=False)\n",
        "print(\"load data successfully\")\n",
        "\n",
        "# ---------- get the features in latent space(features after the layer fc1) for each image ----------\n",
        "features, labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "        # TODO: Get the features of the latent space(after the layer fc1), also gather each image's label\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: concatenate all gathered features and labels, and name them as X and y\n",
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Reduce the dimension of the features to 2, and name the reduced features as Z\n",
        "# Hints: You can use the PCA function you have implemented in Problem 1\n",
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "# ---------- show the latent space in 2D ----------\n",
        "plt.figure(figsize=(7, 6))\n",
        "scatter = plt.scatter(Z[:, 0], Z[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Digit\", bbox_to_anchor=(1.02, 1.0))\n",
        "plt.title(\"PCA of CNN fc1 Feature Map\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "plt.grid(True, linestyle=\":\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890155f7",
      "metadata": {},
      "source": [
        "## Problem 3: K-Means (10 points)\n",
        "\n",
        "In Problem 2, we used PCA to reduce the high-dimensional latent features extracted by a neural network (such as LeNet) to a two-dimensional space, and successfully visualized how the samples are distributed in that space. We observed that samples belonging to the same class tend to cluster closely in the principal component space, suggesting that the extracted features are quite separable.\n",
        "\n",
        "Next, we further explore whether it’s possible to distinguish between different classes using only these low-dimensional representations. To do this, we will apply K-Means clustering on the 2D latent features as an unsupervised learning method. K-Means will attempt to partition all the samples into k=10 clusters, each potentially corresponding to an underlying class.\n",
        "\n",
        "Now, please try implementing it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361388b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from utils import show_cluster\n",
        "\n",
        "\n",
        "def kmeans(X, k=10, max_iter=200, tol=1e-4, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    N = X.shape[0]\n",
        "\n",
        "    # 1. Initialize: Choose k points as initial centers, you can use any method, a random choice is fine\n",
        "    # TODO: Your code here\n",
        "\n",
        "\n",
        "    for t in range(max_iter):\n",
        "        # 2. Assign each point to the closest center\n",
        "        # TODO: Your code here\n",
        "\n",
        "\n",
        "        # 3. visualize the clustering reuslt\n",
        "        if t < 5 or t % 10 == 0:\n",
        "            show_cluster(X, centers, labels, t)\n",
        "\n",
        "        # 4. get the new centers\n",
        "        # TODO: Your code here\n",
        "\n",
        "\n",
        "\n",
        "        # 5. calculate the shift distance of the centers\n",
        "        # TODO: Your code here\n",
        "\n",
        "\n",
        "\n",
        "        # 6. Check whether converged\n",
        "        if shift < tol:\n",
        "            show_cluster(X, centers, labels, t)\n",
        "            print(f\"Converged at iteration {t}\")\n",
        "            break\n",
        "\n",
        "        centers = new_centers\n",
        "\n",
        "    return centers, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dad8f0",
      "metadata": {},
      "source": [
        "Run the K-Means algorithm on the reduced-dim latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0829cac",
      "metadata": {},
      "outputs": [],
      "source": [
        "centers, _ = kmeans(Z)\n",
        "print(centers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CS182",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
