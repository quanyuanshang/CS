\item \defpoints{10} [Linear Regression and Classification]

(a) When we talk about linear regression, what does `linear' regard to? \defpoints{2}

(b) Assume that there are $n$ given training examples $\{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}$, where each input data point $x_i$ has $m$ real valued features. When $m > n$, the linear regression model is equivalent to solving an under-determined system of linear equations $\mathbf{y} = \mathbf{X}\beta$. One popular way to estimate $\beta$ is to consider the so-called ridge regression:
$$\argmin_{\beta} ||\mathbf{y}-\mathbf{X}\mathbf{\beta}||_2^2 + \lambda||\beta||_2^2$$
for some $\lambda > 0$. This is also known as Tikhonov regularization. Show that the optimal solution $\beta^*$ to the above optimization problem is given by
$$\mathbf{\beta}^* = (\mathbf{X}^{\top}\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^{\top}\mathbf{y}$$
Hint: You need to prove that given $\lambda>0$, $\mathbf{X}^{\top}\mathbf{X} + \lambda \mathbf{I}$ is invertible. \defpoints{5}

(c) Is the given data set linear separable? If yes, construct a linear hypothesis function to separate the given data set. If no, explain the reason. \defpoints{3}
\begin{table}[h]
    \centering
    \begin{tabular}{c|cccccc}
        Data & (1,3) & (4,4) & (3,-6) & (-2,1) & (-3,5) & (-6,-4) \\ \hline
        Label & +1 & -1 & -1 & +1 & -1 & -1
    \end{tabular}
    \label{tab:my_label}
\end{table}

\solution









\newpage