{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is one of the most fundamental techniques in machine learning and statistics. It provides a simple yet powerful method for understanding relationships between variables and making predictions. In this assignment, we will explore three different approaches to linear regression:\n",
    "\n",
    " - Least Squares: A method that minimizes the sum of squared residuals.\n",
    "\n",
    " - RANSAC (Random Sample Consensus): A robust estimation technique that mitigates the effect of outliers.\n",
    "\n",
    " - Least Squares as Likelihood Maximization: A probabilistic interpretation of least squares regression based on the assumption of normally distributed errors.\n",
    "\n",
    "By applying these methods to the Boston Housing dataset, students will develop a deeper understanding of regression models and their practical applications.\n",
    "\n",
    "To truly understand the algorithm, **please avoid separating the parameters of W and calculating them individually**. Instead, you should make use of matrix multiplication as much as possible to keep the calculations efficient and aligned with the method's principles. This will also help you better grasp the underlying mechanics of the algorithm.\n",
    "\n",
    "Hint: \n",
    " - Don't worry if your fitted line doesn't perfectly match the data points. As long as it generally follows the overall trend of the data and you have verified that your formula is correct, you're on the right track.\n",
    " - Be careful not to confuse Mean Squared Error (MSE) with the distance from a point to a line. Make sure you understand when to use each metric appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The Boston Housing dataset contains information about housing prices in the Boston metropolitan area. It consists of 506 samples and 13 features, including variables such as crime rate, average number of rooms per dwelling, and distance to employment centers. The goal is to predict the median house price based on these features.\n",
    "\n",
    "The dataset is commonly used to benchmark regression models and analyze feature importance in real-world applications. For this assignment, you will use different regression techniques to fit the data and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define column names for the dataset\n",
    "names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'GE', 'DIS', 'RAD', 'TAX', 'PRTATIO', 'B', 'LSTAT', 'PRICE']\n",
    "\n",
    "# Specify the path to the dataset\n",
    "data_path = 'boston.csv'\n",
    "\n",
    "boston=pd.read_csv(data_path, names=names)\n",
    "\n",
    "# Extract feature variables\n",
    "x_data = boston[names[:-1]]\n",
    "x_data = x_data.to_numpy()[1:].astype(float)\n",
    "\n",
    "# Extract target variable (house price)\n",
    "y_data = boston['PRICE']\n",
    "y_data = y_data.to_numpy()[1:].astype(float)\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization: Display the relationship between each feature and house price.\n",
    "# Each point represents a sample.\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(names[:-1]):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.scatter(x_data[:, i], y_data, s=5)\n",
    "    plt.xlabel(col)\n",
    "    plt.title(f'{col} vs Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, students will explore the relationship between the number of rooms in a house and its price using the **least squares method**. The \"RM\" attribute in the Boston Housing Dataset represents the average number of rooms per dwelling, which is known to be a significant factor in determining house prices.\n",
    "\n",
    "To complete this task, students must manually implement the least squares method to fit a simple linear regression model. **Directly using built-in functions or libraries that solve for the least squares solution is strictly prohibited and will result in a score of zero**!\n",
    "\n",
    "**Please avoid separating the parameters of W and calculating them individually!**\n",
    "\n",
    "Hint: $X^TX$ is invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = y_data\n",
    "X = x_data[:, 5]\n",
    "X = np.vstack([np.ones(len(X)), X]).T\n",
    "W = np.zeros((X.shape[-1], 1))\n",
    "print(X.shape)\n",
    "\n",
    "def compute_MSE(X, y, W):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error (MSE) for the linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    X: A numpy array of shape (N, 2) containing feature matrix (with bias term)\n",
    "    y: A numpy array of shape (N, 1) containing target variable (house price)\n",
    "    W: A numpy array of shape (2,) containing model parameters (weights)\n",
    "\n",
    "    Returns:\n",
    "    float: Mean Squared Error (MSE)\n",
    "    \"\"\"\n",
    "    mse = -1\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return mse\n",
    "\n",
    "def compute_model(X, y):\n",
    "    \"\"\"\n",
    "    Fit a linear regression model using the Least Squares Method.\n",
    "\n",
    "    Parameters:\n",
    "    X: A numpy array of shape (N, 2) containing feature matrix (with bias term)\n",
    "    y: A numpy array of shape (N, 1) containing Target variable (house price)\n",
    "\n",
    "    Returns:\n",
    "    W: A numpy array of shape (2,) containing estimated model parameters (weights)\n",
    "    \"\"\"\n",
    "    W = np.zeros((X.shape[-1], 1))\n",
    "\n",
    "    # Hint: You should use X and y to compute W through matrix multiplication and other operations.\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return W\n",
    "\n",
    "# Run the least squares method to compute model parameters\n",
    "W = compute_model(X, y)\n",
    "print(W)\n",
    "\n",
    "# Generate prediction values for visualization\n",
    "line_X = np.arange(X[:, 1].min(), X[:, 1].max(), 0.1)\n",
    "line_y = W @ np.vstack([np.ones(len(line_X)), line_X])\n",
    "\n",
    "# Visualization of the linear regression fit\n",
    "plt.scatter(X[:, 1], y, s=20, label=\"Data\", color='purple')     # Plot data points\n",
    "plt.plot(line_X, line_y, label=f\"Least Square Method fit: y = {W[1]:.2f}x + {W[0]:.2f}\", color='red')\n",
    "plt.legend()\n",
    "plt.ylim(y.min()-5, y.max()+5)\n",
    "plt.title(\"Least Square Method Linear Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean absolute difference between computed and stored weights\n",
    "# If the implementation is correct, the difference should be close to zero\n",
    "\n",
    "np.savez('check.npz', lstsq=W)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) of your fitted model\n",
    "mse = compute_MSE(X, y, W)\n",
    "print('MSE =', mse)\n",
    "\n",
    "# Load the stored weights and compute the difference\n",
    "data = np.load('check.npz')\n",
    "W_check = data['lstsq']\n",
    "difference = np.abs(W - W_check).mean()\n",
    "print('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we focus on addressing the challenges posed by outliers in linear regression models. While Least Squares Regression is a popular method for fitting a line to data, it is highly sensitive to outliers, which can significantly affect the accuracy of the predictions. The purpose of this task is to introduce you to the RANSAC (Random Sample Consensus) algorithm, a robust approach that minimizes the influence of outliers by iteratively selecting subsets of the data and identifying the best-fitting model based on a threshold of inliers. Through this task, you will learn how to improve the reliability of regression models and make them more robust to data imperfections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_data\n",
    "X = x_data[:, 5]\n",
    "X = np.vstack([np.ones(len(X)), X]).T\n",
    "W = np.zeros((X.shape[-1], 1))\n",
    "print(X.shape)\n",
    "\n",
    "def get_distance(X, y, W):\n",
    "    \"\"\"\n",
    "    Compute the perpendicular distance of each point to the fitted line.\n",
    "    \"\"\"\n",
    "    distances = np.zeros((X.shape[0], 1))\n",
    "\n",
    "    # Hint: You need to compute the perpendicular distance from each point to the fitted line,\n",
    "    # rather than using the residual from the previous step.\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return distances\n",
    "\n",
    "def ransac(X, y, n_iterations=1000, threshold=0.1, num_samples=20):\n",
    "    \"\"\"\n",
    "    Perform RANSAC (Random Sample Consensus) to robustly fit a linear model.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Feature matrix.\n",
    "        y (numpy.ndarray): Target values.\n",
    "        n_iterations (int): Number of iterations for RANSAC.\n",
    "        threshold (float): Distance threshold to determine inliers.\n",
    "        num_samples (int): Number of samples to fit the model.\n",
    "\n",
    "    Returns:\n",
    "        best_model (numpy.ndarray): Best-fitting model weights.\n",
    "        best_inliers (int) : Number of samples that are inliers.\n",
    "    \"\"\"\n",
    "\n",
    "    best_model = None\n",
    "    best_inliers = np.zeros((1))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "\n",
    "        # Hint: First, randomly select num_samples data points and fit a Least Squares model using them.\n",
    "        # Then, count how many points have a distance to the fitted line smaller than the threshold\n",
    "        # (Number of samples that are inliers).\n",
    "        # Finally, choose the model weights that result in the maximum number of inliers as the final prediction.\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return best_model, best_inliers\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "# Run RANSAC algorithm to obtain the best-fitting model and inliers\n",
    "model_weight, best_inliers = ransac(X, y, threshold=threshold)\n",
    "W = model_weight\n",
    "\n",
    "# Generate predictions for visualization\n",
    "line_X2 = np.arange(X[:, 1].min(), X[:, 1].max(), 0.1)\n",
    "line_y2 = W @ np.vstack([np.ones(len(line_X2)), line_X2])\n",
    "\n",
    "# Visualisation\n",
    "plt.scatter(X[:, 1], y, s=20, label=\"Data\", color='purple')         # Original data points\n",
    "plt.plot(line_X2, line_y2, label=f\"RANSAC fit: y = {W[1]:.2f}x + {W[0]:.2f}\", color='orange')\n",
    "plt.plot(line_X, line_y, label=f\"Least Square Method fit: y = {W[1]:.2f}x + {W[0]:.2f}\", color='red')\n",
    "plt.legend()\n",
    "plt.ylim(y.min()-5, y.max()+5)\n",
    "plt.title(\"RANSAC Linear Regression\")\n",
    "plt.show()\n",
    "\n",
    "# Print the number of inliers detected by RANSAC\n",
    "print('The number of inliers detected by RANSAC is:', best_inliers.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares as Likelihood Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares can be interpreted under a probabilistic framework. If we assume that observation noise follows an i.i.d. Gaussian distribution with zero mean and constant variance, maximizing the pribability of the data is equivalent to minimizing ****the distances from all sample points to the regression line****. This provides a theoretical foundation for using Least Squares in regression.\n",
    "\n",
    "This task will help you understand how statistical assumptions shape regression models and highlight the connection between optimization and probability in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Assuming that the distances from all sample points to the regression line follow the same normal distribution, how can we find the line that maximizes the probability of these samples occurring?\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no exact analytical solution to this problem,\n",
    "# you need to define the loss for this problem in `mle_loss(),\n",
    "# and then use scipy.optimize.minimize to optimize the parameters of the line.\n",
    "# What you need to do is compute the loss returned by `mle_loss().\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "y = y_data\n",
    "X = x_data[:, 5]\n",
    "X = np.vstack([np.ones(len(X)), X]).T\n",
    "W = np.zeros((X.shape[-1], 1))\n",
    "print(X.shape)\n",
    "\n",
    "# Define the Maximum Likelihood Estimation (MLE) loss function\n",
    "# based on the perpendicular distance from points to the regression line\n",
    "def mle_loss(W, X, y):\n",
    "    loss = 0.0\n",
    "\n",
    "    # Hint: First, calculate the distance from each point to the line,\n",
    "    #  and then compute the negative log-likelihood of the data.\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Optimize the parameters by minimizing the negative log-likelihood\n",
    "# (which is equivalent to maximizing the likelihood)\n",
    "result = minimize(mle_loss, W.ravel(), args=(X, y), method='L-BFGS-B')\n",
    "\n",
    "# Retrieve the optimal parameters\n",
    "W = result.x\n",
    "print(f\"y = {W[1]:.4f}*x + {W[0]:.4f}\")\n",
    "\n",
    "# Compute the predicted values using the optimized parameters\n",
    "y_pred = W @ X.T\n",
    "\n",
    "print(f\"Final parameters: a={W[1]:.4f}, b={W[0]:.4f}\")\n",
    "\n",
    "# Visualize the data and the estimated regression line\n",
    "# The correct line may appear slightly off, but it should generally have a bottom-left to top-right direction.\n",
    "plt.scatter(X[:, 1], y, color='gray', label='Data')\n",
    "plt.plot(X[:, 1], W @ X.T, color='red', label=f'MLE line: y={W[1]:.2f}x + {W[0]:.2f}')  # Regression line\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.ylim(y.min()-5, y.max()+5)\n",
    "plt.title('MLE Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question 2**\n",
    "\n",
    "Why isn't the predicted line very accurate with this probability-based algorithm?\n",
    "How do you think we can reduce the impact of outliers on this algorithm?\n",
    "If I replace the distance with another distribution, such as a uniform distribution, can we still ensure stable results?\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
